---
title: 'Assessing the Effects of Color and Context on Analytical Problem Solving Ability'
author: 'Maria Auslander, Praveen Joseph, Kevin Jun, Julie Nguyen'
output: 
    github_document: default
    pdf_document: default
knit: (function(inputFile, encoding) {
  rmarkdown::render(
    inputFile, encoding = encoding,
    output_format = c('github_document', 'pdf_document')) 
    })
---

```{r Import libraries, include=FALSE}
library(data.table)
library(lmtest)
library(sandwich)
library(stargazer)
library(dplyr)
library(foreign)
library(knitr)
library(glue)
library(ggplot2)
library(gridExtra)
```

```{r,include=FALSE}

Robust_SE = function(model){
  
  Robust_SE = sqrt(diag(vcovHC(model)))
  
  return(Robust_SE)
}

Clustered_SE = function(model, cluster){
  
  Clustered_SE = sqrt(diag(vcovCL(model, cluster) ))
  
  return(Clustered_SE)
}


CI = function(model){
  
  coeftest(model, vcov. = vcovHC)

  ate = model$coef[2]
  SE_ate =   sqrt(vcovHC(model)[2,2])
  
  CI = c(ate-1.96*SE_ate, ate+1.96*SE_ate)
  
  return(CI)
  
}

CI_cluster= function(model,cluster) {
  critical_val=qnorm(1-.05/2)
  inter=model$coef[1]
  ate=model$coef[2]
  
  SE=Clustered_SE(model=model,cluster=cluster)
  SE_inter=SE[1]
  SE_ate=SE[2]
  
  lwr<-(inter-critical_val*SE_inter)+(SE_ate-critical_val*SE_ate)
  upr<-(inter+critical_val*SE_inter)+(SE_ate+critical_val*SE_ate)
  
  CI_cluster= c(lwr,upr)
  return(CI_cluster)
  
}



p_value = function(model){
  
  p_value = coeftest(model, vcov. = vcovHC(model))[2,4]
  
  return(p_value)
}
```

# Research Question

This paper seeks to examine the research question, "Can the use of color and/or the use of questions with context help students improve analytical problem solving performance?". In order to address this question, a 2 x 3 factorial design with a within-subjects component is utilized. Each subject is asked six unique analytical problem solving questions each with 2 potential color conditions– 'no color' or 'with color', and 3 potential context conditions– 'question without context', 'question with context', and 'question with context and diagram'. We took steps to mitigate the carryover and learning effect of within subject studies, by using counterbalancing. The treatment conditions are assigned randomly per question and the order of each question is randomized. We discovered that the Color conditions were not found to be significant in improving the outcome of analytical problem solving questions. The third context condition, question context with diagram, had a significant positive effect on the question score outcome in some cases, however, there is a possibility that a treatment effect does not exist for this treatment condition either.

# Introduction / Motivation

The topic for this experimental study bears resemblance to an influential study conducted in 1956 by George A. Miller - a cognitive psychologist from Harvard University's Department of Psychology. This study on human cognition led to Miller’s law in psychology. The study titled "The Magical Number Seven, Plus or Minus Two: Some Limits on Our Capacity for Processing Information" is one of the most highly cited papers in psychology published in Psychological Review (1956).

Our study follows the preamble of Miller’s study and seeks to answer questions of human cognition when participants in the study are subject to treatment with colour. We would then devise a method to measure and test the cognitive capacity for processing (analyzing) and remembering through the process of learning- information. 

The reason why this is an interesting experiment to design and execute is because the results could have a meaningful impact for delivering learning materials. As online learning becomes more prevalent, we seek to answer a pertinent question through this study; Can the use of color and/or the use of questions with context help students improve analytical problem solving performance?


# Experimental Design

Our experiment aims to see if there is any significant improvement in analytical problem solving by (a) adding color, (b) adding context to the question, and/or (c) implementing both onto a straightforward black and white baseline question format. 

The first arm of treatment is to add 'color', in which text-only problems are now shown on a cyan background with white letters and problems with diagrams are shown in their full colored version opposed to grey-scale. Of note, the color cyan was selected randomly out of many purely to test the hypothesis and because it is distinguishable to color-blind respondents, if any.

\begin{figure}[h!]
\includegraphics[width=5cm, height=5cm]{questions/Q5_Condition3_BW.png}
\caption{Sample Question Without Color, and Context plus Diagram}
\end{figure}

\begin{figure}[h!]
\includegraphics[width=5cm, height=5cm]{questions/Q5_Condition3_Color.png}
\caption{Sample Question With Color, and Context plus Diagram}
\end{figure}

Here, the expectation is that the colored version of the question will drive more engagement from the viewer and act as a better stimulus to understand the problem and hence solve it better.

The next arm of the treatment conditions is adding context to questions. The context ‘treatment’ consists of two steps: the first is by adding situational context to the question.
\newpage
\begin{figure}[h!]
\includegraphics[width=5cm, height=5cm]{questions/Q5_Condition1_BW.png}
\caption{Sample Question Without Color and Without Context}
\end{figure}

\begin{figure}[h!]
\includegraphics[width=5cm, height=5cm]{questions/Q5_Condition2_BW.png}
\caption{Sample Question Without Color and With Context}
\end{figure}


This part of the experiments benchmarks from the infamous 'selection task' devised by Peter Cathcart Wason in 1966. In this task, subjects are shown a set of 4 cards on the table, each of which has a figure on one side and a colored patch on the other side. People have to answer the logical question following: "Which cards must be turned over to test the idea that if a card shows an even number on one face, then the opposite face is red?" In this test, less than 10% of the subjects found the correct solution. However, subjects find the task much easier to solve if it is placed in the context of a social rule that they are asked to enforce. One of these experiments (Cosmides & Tooby, 1992) is using a set of 4 other cards on the table: two card have an age on one side and beverage on the other, e.g., "16", "25", "coke", "beer". When being asked "Which cards must be turned over to test the idea that if you are drinking alcohol then you must be over 18?". Most people correctly pick the correct cards ("16" and "beer"). Cosmides and Tooby's experiment supports Watson's assumption that the Watson selection task is highly content-dependant. 

The key idea is similar in our case too, in that layering a relatable context to the analytical problem is intended to aid the participant. Like in the example illustrated above, instead of listing down a set of numbers, we put them into context as 'six-sided dice', 'prices of items', 'cards', 'people', or 'pets'. By giving tangible examples, the expectation is that the respondent will be able to understand the question better.

The second step of the context treatment is, adding to the context-treated problem a 'diagram' that visually illustrates the problem. 

\newpage
\begin{figure}[h!]
\includegraphics[width=5cm, height=5cm]{questions/Q5_Condition3_BW.png}
\caption{Sample Question Without Color and with Context plus Diagram}
\end{figure}

This stimuli is expected to provide visual cues to understand the question even more easily.



Since we are looking at two vectors of treatment, our NULL hypothesis can be divided into two:

> 1. *Color* has no effect on a subjects' ability to answer an analytical question
> 2. *Additional context* to the question has no effect on subjects' ability to answer an analytical question

In order to test this hypothesis, a 2 x 3 factorial design with a within-subjects component is utilized. Each subject is asked six unique analytical problem solving questions each with 2 potential color conditions– 'no color' or 'with color', and 3 potential context conditions– 'question without context', 'question with context', and 'question with context plus diagram'.

```{r Experimental design, echo=FALSE}
table <- data.frame(matrix(vector(), 2, 3,
                           dimnames = list(c("No Color", "With Color"), 
                                           c("No Context", "With Context", "Context and Diagram"))))
table[1, 1] <- "Control (A-0)"
table[1, 2] <- "Treatment (A-1)"
table[1, 3] <- "Treatment (A-2)"
table[2, 1] <- "Treatment (B-0)"
table[2, 2] <- "Treatment (B-1)"
table[2, 3] <- "Treatment (B-2)"

kable(table)
```


For each participant, we ask a set of different analytical problem solving questions, each randomly assigned one of the six conditions from our 2x3 design, with replacement. We also randomize the order of the questions to even out any ‘training effect’ where respondents could increasingly become better at solving math problems throughout the survey itself. Lastly, participants were offered a reward for scoring above a certain threshold (*getting an overall score of 5 or higher*), as an incentive to do well on the problems presented in the study and avoid non-compliance (where a subject could simply click random answers to the questions without actually making an effort to solve them).


# Pilot Review

A pilot study was conducted in order to determine the feasibility and data collection practices for the final study. The pilot consisted of two surveys sent to differing groups--one to friends and family, and another to subjects through prolific and google surveys. The surveys were created through Qualtrics - an online survey platform. Each survey had three questions and each possible color and context combination of conditions was tested through the surveys. Every subject completed three analytical problem solving questions and at the end of the survey each subject was asked demographic questions as well as questions on the difficulty of the problem-solving tasks. The aim of asking demographic questions was to test the effect of covariates on outcomes. Because our experiment consists of a within-subjects design, we wanted the questions to be of similar difficulty coming out of the pilot study and into the full study to make a more clear comparison of outcomes amongst differing questions.


```{r pilot,include=FALSE}
v1 <- fread("./pilot_v1.csv")
v2 <- fread("./pilot_v2.csv")
v1<-v1 %>% slice(-c(1,2))
v2<-v2 %>% slice(-c(1,2))
v1<-v1[v1$Status=='IP Address',]
v2<-v2[v2$Status=='IP Address',]


v1$q1_color<-1
v1$q2_color<-0
v1$q3_color<-0

v2$q1_color<-1
v2$q2_color<-0
v2$q3_color<-1

v1$q1_condition<-1
v1$q2_condition<-2
v1$q3_condition<-3

v2$q1_condition<-2
v2$q2_condition<-1
v2$q3_condition<-3

v1$q1_correct<-ifelse(v1$Q1=="9/20",1,0)
v1$q2_correct<-ifelse(v1$Q2=="1/9",1,0)
v1$q3_correct<-ifelse(v1$Q3=="1/2",1,0)

v2$q1_correct<-ifelse(v2$Q1=="9/20",1,0)
v2$q2_correct<-ifelse(v2$Q2=="1/2",1,0)
v2$q3_correct<-ifelse(v2$Q3=="1/9",1,0)

#Rename columns
v1 <- v1 %>% rename(age = Q10, gender = Q11,education=Q12, stated_question_difficulty=Q14)
v2 <- v2 %>% rename(age = Q5, gender = Q6,education=Q7, stated_question_difficulty=Q9)

df1<-NA
for (q in 1:3) {
  question_df<-v1[,ResponseId,c('age','gender','education',glue("Q{q}"),glue('q{q}_correct'),glue('q{q}_color'),glue('q{q}_condition'))]
  
  question_df <- question_df %>% rename("answer"=glue("Q{q}"),"correct"=glue('q{q}_correct'),"color"=glue('q{q}_color'),'condition' = glue('q{q}_condition'))
  
  question_df$question_number=q
  
  if (is.na(df1)) {
    df1=question_df
  } else {
    df1=rbind(df1,question_df,fill=TRUE)
  }
}


df2<-NA
for (q in 1:3) {
  question_df<-v2[,ResponseId,c('age','gender','education',glue("Q{q}"),glue('q{q}_correct'),glue('q{q}_color'),glue('q{q}_condition'))]
  
  question_df <- question_df %>% rename("answer"=glue("Q{q}"),"correct"=glue('q{q}_correct'),"color"=glue('q{q}_color'),'condition' = glue('q{q}_condition'))
  
  question_df$question_number=q
  
  if (is.na(df2)) {
    df2=question_df
  } else {
    df2=rbind(df2,question_df,fill=TRUE)
  }
}
head(df2)

df1$color_factor<-as.factor(df1$color)
df1$condition_factor<-as.factor(df1$condition)
df1$question_number_factor<-as.factor(df1$question_number)

df2$color_factor<-as.factor(df2$color)
df2$condition_factor<-as.factor(df2$condition)
df2$question_number_factor<-as.factor(df2$question_number)

shared_df=rbind(df1,df2)
```

### Pilot Study Results

```{r pilot response rate,include=FALSE}
#repsonse rate per question
for (n in 1:3) {
  response_rate<-length(df1[df1$question_number==n & df1$answer!="",]$answer)/length(df1[df1$question_number==n,]$answer)
  print(glue("Question {n}"))
  print(response_rate[1])
}

for (n in 1:3) {
  response_rate<-length(df2[df2$question_number==n & df2$answer!="",]$answer)/length(df2[df2$question_number==n,]$answer)
  print(glue("Question {n}"))
  print(response_rate[1])
}
```

As part of our analysis on the pilot study, we reviewed the response rate for questions. The response rate by question number per survey is listed below:

| Study Version | Question Number | Response Rate |
|---------------|-----------------|---------------|
| 1             | 1               | 0.9615385     |
| 1             | 2               | 0.9615385     |
| 1             | 3               | 0.9615385     |
| 2             | 1               | 0.7142857     |
| 2             | 2               | 0.8571429     |
| 2             | 3               | 0.8571429     |


Study version 1 was sent to respondents on qualtrics and google surveys whereas study version 2 was sent only to friends and family. Because the analytical problem solving questions were not a requirement to submit the survey, users could submit the survey without answering questions. Looking at the table above, it is clear that question 1 from study version 2 had the lowest response rate. Additionally we received a higher frequency of comments on the difficulty of this question than any other question listed. Here is an example of one such statement when a subject was asked to comment on the difficulty of questions, "First question was impossible to comprehend". Upon further investigation we came to the conclusion that question 1 was worded poorly, hence we decided to refrain from using this question in the final study. Question 1 of the pilot study is listed in the appendix under A7.

Further analysis is conducted below on the inclusion of questions. Models are listed below per question number which regress dummy variables indicating whether the question number was the one of interest or not against the `correct` variable, indicating whether a question was correct. A sample formula is below (with a dummy variable for question 1):\  
`correct`~`question_number==1`

The models are created using clustered standard errors, clustering on `ResponseId`, a variable which indicates a specific respondent. Clustered standard errors are used to control for differences among subjects. The results for study version 1 models are below, where the models are listed in order of question number.

```{r,echo = FALSE}
s1.q1.mod<-df1[,lm(correct~question_number_factor==1)]
s1.q2.mod<-df1[,lm(correct~question_number_factor==2)]
s1.q3.mod<-df1[,lm(correct~question_number_factor==3)]

stargazer(s1.q1.mod,s1.q2.mod,s1.q3.mod,
          type="text", 
          se=list(c(Clustered_SE(model=s1.q1.mod,cluster= df1[ , ResponseId]),
                    Clustered_SE(model=s1.q2.mod,cluster= df1[ , ResponseId]),
                    Clustered_SE(model=s1.q3.mod,cluster= df1[ , ResponseId])
                    )))
```

From the results above, we concluded that question 1 has a significant, negative effect on whether a question is answered correctly. Questions 2 and 3 do not have significant effects on whether a question is answered correctly. Question 1 in study version 1 is not used in the final study. The results for study version 2 models are in A10 of the appendix. In the results for study version 2, it is evident that question 1 has a significant, negative effect on whether a question is answered correctly. Questions 2 and 3 do not have significant effects on whether a question is answered correctly. It is also notable that the low response rate for question 1 in study version 2 may make the comparison of outcomes for question 1 with the other question outcomes a less valid comparison, this is under the implication that subjects may have been less likely to answer questions they did not understand.

In determining the questions to include in the final study, in addition to comparing question results, we also want to avoid ceiling or floor effects. According to Garin, "The ceiling effect is said to occur when participants’ scores cluster toward the high end (or best possible score) of the measure/instrument. The opposite is the floor effect." A ceiling effect can create an issue where a normal distribution predicts outcomes above the maximum level where a floor effect can create an issue where a normal distribution predicts outcomes below the minimum level. To avoid ceiling and floor effects, we examined questions to determine if too high a proportion of subjects were answering a question correctly or incorrectly. The same models listed above regressing question number dummy variables on `correct` (example: correct ~ question number == 1) can be used to assess the likelihood of ceiling and floor effects. 

```{r,include=FALSE}
s1.q1.mean<-round(mean(df1[question_number_factor==1,]$correct),4)
s1.q2.mean<-round(mean(df1[question_number_factor==2,]$correct),4)
s1.q3.mean<-round(mean(df1[question_number_factor==3,]$correct),4)

s2.q1.mean<-round(mean(df2[question_number_factor==1,]$correct),4)
s2.q2.mean<-round(mean(df2[question_number_factor==2,]$correct),4)
s2.q3.mean<-round(mean(df2[question_number_factor==3,]$correct),4)
```

Below the proportions of users answering questions correctly and incorrectly by study number and question number are listed. Ultimately, to avoid ceiling effects it would be best if the proportion of subjects who answered a question correctly at around 50% and the proportion of subjects who answered a question incorrectly at around 50% as well. Looking at the table below, we do not see any overt cases for a question having a ceiling or floor effect (<20% correct or <20% correct). However, question 1 in study 1 has a fairly low percentage of subjects who've answered the question correctly (`r s1.q1.mean*100` %) and question 3 in study 2 has a fairly high percentage of subjects who've answered the question correctly  (`r s2.q3.mean*100` %) .

| Study Version | Question Number | Proportion Correct| Proportion Incorrect|
|---------------|-----------------|-------------------|---------------------|
| 1             | 1               | `r s1.q1.mean`| `r 1-s1.q1.mean`|
| 1             | 2               | `r s1.q2.mean`| `r 1-s1.q2.mean`|
| 1             | 3               | `r s1.q3.mean`| `r 1-s1.q3.mean`|
| 2             | 1               | `r s2.q1.mean`| `r 1-s2.q1.mean`|
| 2             | 2               | `r s2.q2.mean`| `r 1-s2.q2.mean`|
| 2             | 3               | `r s2.q3.mean`| `r 1-s2.q3.mean`|


After assessing results on the question level to determine which questions are appropriate for the final study, we look to determine whether the randomization method implemented by Qualtrics is viable. Because we did not develop the software/code that Qualtrics uses to randomize treatment, we created models for each color treatment condition and each treatment condition which regress covariates against treatment conditions to ensure covariate values did not affect treatment. The simple formulas for the models described are as follows (without coefficients):

* color ~ gender + education +age
* context condition ~ gender
* color ~ age
* context condition ~ age
* color ~ education
* context condition ~ education

There is no reason to believe randomization of treatment conditions would be affected by Qualtrics software, Prolific or Google Surveys, however, because we did not design the software ourselves, we've decided to check covariates have no effect on treatments assigned. The models described for gender are listed below, showing no significant effects of covariates on treatment conditions. The models for age and education are in A11 of the appendix. As with the gender models, no covariates had a significant effect on the treatment conditions assigned.

```{r,echo=FALSE}
for (cov in c('gender')) {
  cov=glue("{cov}")
  color_mod<-lm(shared_df$color~shared_df[[cov]])#+shared_df$ResponseId)
  condition_mod<-lm(shared_df$condition~shared_df[[cov]])#+shared_df$ResponseId)
  stargazer(title=glue("{cov} models:"),color_mod,condition_mod, dep.var.labels  = c("color","context"),
            omit="ResponseId",covariate.labels = c("Female","Male","Constant"),
            type="text")
  }
```

Looking at the outcomes above, no covariates have a significant effect on treatment condition selection. We assume the randomization of treatment leads to expected balance on covariates.

A short analysis of power based on pilot results is displayed below. However, the results in our pilot are not expected to perfectly match the results of the final study, particularly taking into account differences in outcomes (question score) by question number where the questions in the final study will not perfectly match those of the pilot. According to the results of the pilot study, in order to have 80% power, there will need to be about 430 samples per condition. In the final study, as many subjects as possible are recruited as according to budgetary constraints. The lines represent the point at which 80% power will be achieved.

```{r,include=FALSE}

# To use for power calculation
shared_df[, .('mu_score' = mean(correct)), keyby = .(condition_factor, color)]

score_sd_1 = sd(shared_df[condition_factor==1 & color == 0, correct])
score_sd_3 = sd(shared_df[condition_factor==3 & color == 1, correct])

print(score_sd_1)
print(score_sd_3)
```

```{r power function PILOT, include=FALSE,warning=FALSE,echo=FALSE}
power_test_t <- function(
  mean_control = 0.6190476, 
  mean_treat = 0.7142857, 
  sd_control = 0.4976134, 
  sd_treat = 0.46291,
  number_per_condition = 2000, 
  power_loops = 1000, 
  ri_loops = 1000, 
  verbose = TRUE) { 

    p_values <- NA  
    ri <- NA 
    d <- data.table('id'= 1:(number_per_condition*2))
    
    #d$condition=rep(c('control', 'treatment'), each = number_per_condition)
  
    d[, condition := rep(c('control', 'treatment'), each = number_per_condition)]  
  
    for(power_loop in 1:power_loops) { 
      if(verbose == TRUE) {
        if(power_loop %% 10 == 0) {
          cat(sprintf('Loop Number: %.0f\n', power_loop))
        }
      } 
      
      p_values[power_loop] <- t.test(
        x = rnorm(number_per_condition, mean = mean_control, sd = sd_control), 
        y = rnorm(number_per_condition, mean = mean_treat, sd = sd_treat)
      )$p.value
    }
      
    return(list(
      'p_values' = p_values, 
      'power' = mean(p_values < 0.05)
      ))
}
```


```{r message=FALSE, warning=FALSE,echo=FALSE,results='hide',fig.keep='all'}

# Increasing sample size
samples_per_condition <- c(10, 20, 50, 100, 150, 200, 260, 300, 500, 750, 1000, 1500, 2000, 2500, 3000)

size_power <- NA 

for(i in 1:length(samples_per_condition)) { 
  size_power[i] <- power_test_t(
    mean_control = 0.6190476 , mean_treat = 0.7142857,
    power_loops = 1000, verbose = FALSE,
    number_per_condition = samples_per_condition[i]
    )$power
}

plot(x = samples_per_condition, y = size_power, type = 'l', 
     main = "Sample Size and Power", ylab = "Power", xlab = "Samples per Condition") +
  abline(h = 0.8, col = 'blue') +
  abline(v = 430, col = 'blue')
```

# Final Research Methodology

### Experimental Design Review

As previously mentioned, the final version of the study consists of a 2x3 factorial design where treatment can have one of two color conditions and one of three question context conditions. The two color conditions are without color (control) and with color. The three question context conditions are: without context (control), with context, and with context plus diagram.  Six questions were selected to be used in assessing the effects of treatment conditions on subjects. These questions, each with all available conditions listed, are available in the appendix (A1-A6).

### Subject Recruitment and Treatment Randomization

The research design has a within-subjects component--each subject is asked all six potential questions with differing, randomized treatment conditions in a randomized order. An illustration of our treatment randomization is below.

\begin{figure}[h!]
\includegraphics{treatment_randomization.jpg}
\caption{Treatment Randomization Illustration}
\end{figure}

Based on the results of the pilot study, the final study was implemented using Qualtrics to create the survey and Prolific - an online participant recruitment platform - to recruit users to the survey. The experiment was limited in budget, so the sample size was determined based on what was afforded within this experimental constraint. We analyzed results from 260 subjects recruited through prolific. There was an initial pool of 132,393 subjects in Prolific. The total number of potential subjects was reduced to 101,740 when a condition was placed where subjects needed to be fluent in English in order to understand the questions at hand, and total number of potential subjects was reduced again to 93,903 when only subjects with an Prolific approval rate greater than or equal to 50% were selected. 260 subjects were analyzed from the total pool of 93,903 subjects; these subjects were selected randomly. 266 subjects were initially selected for treatment but 6 subjects revoked consent. A consort diagram detailing subject selection is below.

\begin{figure}[h!]
\includegraphics[width=8cm,height=12cm]{consort_v2.jpg}
\caption{Consort Diagram}
\end{figure}
\newpage

### Outcome Measurement

In this study, each question answered by a subject consists of an observation and whether the question is asnwered correctly or not is the measured outcome. In the analysis the outcome variable (a boolean indicating whether a question was answered correctly) is listed as `score`. Potential treatment conditions whose outcomes are assessed are listed below:

```{r echo=FALSE}
table <- data.frame(matrix(vector(), 2, 3,
                           dimnames = list(c("No Color", "With Color"), 
                                           c("No Context", "With Context", "Context and Diagram"))))
table[1, 1] <- "Control (A-0)"
table[1, 2] <- "Treatment (A-1)"
table[1, 3] <- "Treatment (A-2)"
table[2, 1] <- "Treatment (B-0)"
table[2, 2] <- "Treatment (B-1)"
table[2, 3] <- "Treatment (B-2)"

kable(table)
```


```{r Import data, include=FALSE}
# Read in the data exported from each platform (prolific and qualtrics)
d1 <- fread("./colors_prolific.csv")
d2 <- fread("./colors_qualtrics.csv")
d3 <- fread("./colors_qualtrics_with_order.csv")

# Remove tests and empty rows (previews)
d2 <- d2[PROLIFIC_PID != '5f1745f03565ea0a6d4ff1a1' & PROLIFIC_PID != ""]    

# Left-join qualtrics data (d2) onto prolific respondents (d1) using participant_id
d <- merge(d1, d2, all.x = TRUE, by.x = 'participant_id', by.y = 'PROLIFIC_PID')

d3<-d3[,c('PROLIFIC_PID','block_order_1','block_order_2',
          'block_order_3','block_order_4',
          'block_order_5','block_order_6')]

d<-merge(d,d3,all.x=TRUE, by.x = 'participant_id', by.y = 'PROLIFIC_PID')

# Filtering for only subjects who were approved in the study
d <- d[d$status == "APPROVED" | d$status == "REJECTED" , ]

head(d)
```



```{r Data scrubbing, include=FALSE}

# Summarizing separated question variation responses
d$Question_1 <- trimws(paste(d$Q21, d$Q22, d$Q23, d$Q24, d$Q25, d$Q26))
d$Question_2 <- trimws(paste(d$Q27, d$Q28, d$Q29, d$Q30, d$Q31, d$Q32))
d$Question_3 <- trimws(paste(d$Q33, d$Q34, d$Q35, d$Q36, d$Q37, d$Q38))
d$Question_4 <- trimws(paste(d$Q39, d$Q40, d$Q41, d$Q42, d$Q43, d$Q44))
d$Question_5 <- trimws(paste(d$Q45, d$Q46, d$Q47, d$Q48, d$Q49, d$Q50))
d$Question_6 <- trimws(paste(d$Q51, d$Q52, d$Q53, d$Q54, d$Q55, d$Q56))

d$q1_correct <- ifelse(d$Question_1 == "1/9", 1, 0)
d$q2_correct <- ifelse(d$Question_2 == "15/32", 1, 0)
d$q3_correct <- ifelse(d$Question_3 == "1/10", 1, 0)
d$q4_correct <- ifelse(d$Question_4 == "E", 1, 0)
d$q5_correct <- ifelse(d$Question_5 == "3/8", 1, 0)
d$q6_correct <- ifelse(d$Question_6 == "S", 1, ifelse(d$Question_6 == "Swati", 1, 0))

# Set up booleans for color treatment
d$q1_color <- ifelse(trimws(paste(d$Q22, d$Q25, d$Q26)) != "", 1, 0)
d$q2_color <- ifelse(trimws(paste(d$Q28, d$Q30, d$Q32)) != "", 1, 0)
d$q3_color <- ifelse(trimws(paste(d$Q34, d$Q36, d$Q38)) != "", 1, 0)
d$q4_color <- ifelse(trimws(paste(d$Q40, d$Q42, d$Q44)) != "", 1, 0)
d$q5_color <- ifelse(trimws(paste(d$Q46, d$Q48, d$Q50)) != "", 1, 0)
d$q6_color <- ifelse(trimws(paste(d$Q52, d$Q54, d$Q56)) != "", 1, 0)

# Set up booleans for various treatment conditions
d$q1_cond1 <- ifelse(trimws(paste(d$Q21, d$Q22)) != "", 1, 0)
d$q2_cond1 <- ifelse(trimws(paste(d$Q27, d$Q28)) != "", 1, 0)
d$q3_cond1 <- ifelse(trimws(paste(d$Q33, d$Q34)) != "", 1, 0)
d$q4_cond1 <- ifelse(trimws(paste(d$Q39, d$Q40)) != "", 1, 0)
d$q5_cond1 <- ifelse(trimws(paste(d$Q45, d$Q46)) != "", 1, 0)
d$q6_cond1 <- ifelse(trimws(paste(d$Q51, d$Q52)) != "", 1, 0)

d$q1_cond2 <- ifelse(trimws(paste(d$Q23, d$Q25)) != "", 1, 0)
d$q2_cond2 <- ifelse(trimws(paste(d$Q29, d$Q30)) != "", 1, 0)
d$q3_cond2 <- ifelse(trimws(paste(d$Q35, d$Q36)) != "", 1, 0)
d$q4_cond2 <- ifelse(trimws(paste(d$Q41, d$Q42)) != "", 1, 0)
d$q5_cond2 <- ifelse(trimws(paste(d$Q47, d$Q48)) != "", 1, 0)
d$q6_cond2 <- ifelse(trimws(paste(d$Q53, d$Q54)) != "", 1, 0)

d$q1_cond3 <- ifelse(trimws(paste(d$Q24, d$Q26)) != "", 1, 0)
d$q2_cond3 <- ifelse(trimws(paste(d$Q31, d$Q32)) != "", 1, 0)
d$q3_cond3 <- ifelse(trimws(paste(d$Q37, d$Q38)) != "", 1, 0)
d$q4_cond3 <- ifelse(trimws(paste(d$Q43, d$Q44)) != "", 1, 0)
d$q5_cond3 <- ifelse(trimws(paste(d$Q49, d$Q50)) != "", 1, 0)
d$q6_cond3 <- ifelse(trimws(paste(d$Q55, d$Q56)) != "", 1, 0)

# Renaming ambiguous fields
names(d)[names(d) == "Q13"] <- "time_declared"
names(d)[names(d) == "Q12"] <- "Education"

```


```{r Creating data table for analysis, include=FALSE}
dt <- NA

for (q in 1:6) {
  question_dt <- d[ , c('participant_id',
                        glue('q{q}_color'),
                        glue('q{q}_cond1'), glue('q{q}_cond2'), glue('q{q}_cond3'),
                        glue("Question_{q}"), glue('q{q}_correct'),glue("block_order_{q}"),
                        'time_taken', 'time_declared',
                        'age', 'Sex', 'Education', 'Student Status',
                        'Employment Status', 'Nationality'
                        )]
  
  question_dt <- question_dt %>%
    rename("answer" = glue("Question_{q}"),
           "score" = glue('q{q}_correct'),
           'question_order'=glue('block_order_{q}'),
           "color" = glue('q{q}_color'),
           'cond1' = glue('q{q}_cond1'), 
           'cond2' = glue('q{q}_cond2'), 
           'cond3' = glue('q{q}_cond3'),
           'sex'   = 'Sex',
           'education' = 'Education',
           'student_status' = 'Student Status',
           'employment_status' = 'Employment Status',
           'nationality' = 'Nationality'
           )
  
  question_dt$question_number <- q
  
  if (is.na(dt)) {dt <- question_dt}
  else {dt <- rbind(dt, question_dt, fill=TRUE)}
}

# Creating question conditions and factors for later analysis
dt$question_condition <- ifelse(dt$cond1 == 1, 1,
                                ifelse(dt$cond2 == 1, 2,
                                       ifelse(dt$cond3 == 1, 3, 0)))
dt$factor_color <- as.factor(dt$color)
dt$factor_context <- as.factor(dt$question_condition)

# Aggregating education levels for simplicity
dt[, edu_group := ifelse(education %in% c("High school graduate", 
                                           "Completed some high school"), "Highschool", 
                          ifelse(education %in% c("Associate degree", 
                                                  "Bachelor's degree", 
                                                  "Completed some college"), "College", 
                                 ifelse(education %in% c("Completed some postgraduate",
                                                         "Master's degree",
                                                         "Other advanced degree beyond a Master's degree",
                                                         "Ph.D., law or medical degree"), "Mast/PhD", "")))]

# Assigning id numbers to simplify name vs. participant_id
dt <- dt[order(participant_id)]
dt[, id := rep(1:(.N/6), each = 6)]

# Reordering columns
col_names <- colnames(dt)
setcolorder(dt, c("id", "score", "color", "cond1", "cond2", "cond3",
                  "question_number", "question_condition","question_order", "factor_color", "factor_context",
                  "age", "sex", "time_taken", "time_declared", "education", "edu_group", "student_status",
                  "employment_status", "nationality", "answer", "participant_id"
                  ))
```


# Exploratory Data Analysis (EDA)

IN t clearly detailed, and the reader can understand at a conceptual and operational level.

A total of `r dt[, uniqueN(id)]` unique respondents completed our survey on prolific, the demographic was skewed towards younger respondents spread across different education levels (and nationalities mainly European, led by UK, Poland, and Portuguese). (We also saw that roughly half of all respondents are currently students, more prominently in Poland and Portugal.)
\newpage

```{r EDA variable overview, echo=FALSE, message=FALSE}
# unique respondents: 260
# dt[, uniqueN(id)]
`%notin%` <- Negate(`%in%`)
dt_cov <- unique(dt[, .(age, sex, nationality, education, edu_group, student_status, employment_status), keyby = id])

# age & sex
gg_age_sex <- ggplot(dt_cov, aes(x = age, fill = sex)) + 
                geom_bar(data = subset(dt, sex == "Female")) + 
                geom_bar(data = subset(dt, sex == "Male"), aes(y = ..count..*(-1))) + 
                labs(subtitle = "Demographics (Age & Sex)", x = "Age", y = "Count") + coord_flip()

# summarized nationality
# dt[ , top_nationalities := ifelse(nationality %in% c('United Kingdom', 'Poland', 'Portugal', 
#                                                            'Italy', 'United States'
#                                                            ), nationality, "Other")]
# dt_cov[ , top_nationalities := ifelse(nationality %in% c('United Kingdom', 'Poland', 'Portugal', 
#                                                            'Italy', 'United States'
#                                                            ), nationality, "Other")]
# 
# ggplot(dt_cov[student_status %in% c("Yes", "No")], aes(x = top_nationalities, fill = student_status)) + 
#   geom_bar(stat = "count") + 
#   labs(subtitle = "Nationalities and Student Status", x = "Nationality", y = "Count") + coord_flip()

# education & student status
gg_edu_stud <- ggplot(dt_cov[student_status %in% c("Yes", "No") & education !=""], 
                      aes(x = edu_group, fill = student_status)) + 
                 geom_bar(position = "stack") + coord_flip() +
                 labs(subtitle = "Education Levels & Student Status", x = "Education Level", y = "Count")

# employment
# ggplot(dt_cov[student_status %in% c("Yes", "No")], aes(x = employment_status, fill = student_status)) + 
#   geom_bar(stat = "count") + 
#   labs(subtitle = "Employment & Student Status", x = "Emp Status", y = "Count") + coord_flip()
# dt[ , employment_status, keyby = employment_status]

grid.arrange(gg_age_sex, gg_edu_stud, ncol=2)

```


Through the Qualtrics survey platform, we were able to programmatically randomize the distribution of each condition of our treatment. For reassurance, we next visually check to see if either the color treatment arm or context treatment arm was skewed toward a certain covariate. The plots below show no major imbalances between treatment conditions across gender, age,  education level, as the heights of each bar are similar within each breakout.

```{r EDA Covariate balance checks, echo=FALSE, message=FALSE}
# Pre-calculating mean age by treatment factor
cbc_color_age <- dt[, .(mean(age)), keyby = factor_color][, V1]
cbc_context_age <- dt[, .(mean(age)), keyby = factor_context][, V1]

# Covariate balance check of treatment: Sex distribution by treatment
gg_sex_color <- dt[sex != ""] %>%
  ggplot(aes(x = sex, fill = factor_color)) + 
    geom_bar(position = "dodge") + labs(x = "Sex", y = "Count") +
    theme(legend.position = "left", axis.text.x = element_text(angle = 45))

gg_sex_context <- dt[sex != ""] %>%
  ggplot(aes(x = sex, fill = factor_context)) + 
    geom_bar(position = "dodge") + labs(x = "Sex", y = "Count") +
    theme(legend.position = "left", axis.text.x = element_text(angle = 45))

# Covariate balance check of treatment: Age distribution by treatment
gg_age_color <- dt %>%
  ggplot(aes(x = age, fill = factor_color)) + 
    geom_histogram(position = position_dodge(width = 7), binwidth = 10) +
    geom_vline(xintercept = c(cbc_color_age[1], cbc_color_age[2]), linetype = "dotted") +
    labs(x = "Age Group", y = "Count") +
    theme(legend.position = "none")
  
gg_age_context <- dt %>%
  ggplot(aes(x = age, fill = factor_context)) + 
    geom_histogram(position = position_dodge(width = 7), binwidth = 10) +
    geom_vline(xintercept = c(cbc_context_age[1], cbc_context_age[2], cbc_context_age[3]), linetype = "dotted") +
    labs(x = "Age Group", y = "Count") +
    theme(legend.position = "none")

# Covariate balance check of treatment: Education level by treatment
gg_edu_color <- dt[edu_group != ""] %>%
  ggplot(aes(x = edu_group, fill = factor_color)) + 
    geom_bar(position = "dodge") + labs(x = "Education Level", y = "Count") +
    theme(legend.position = "none", axis.text.x = element_text(angle = 45))

gg_edu_context <- dt[edu_group != ""] %>%
  ggplot(aes(x = edu_group, fill = factor_context)) + 
    geom_bar(position = "dodge") + labs(x = "Education Level", y = "Count") +
    theme(legend.position = "none", axis.text.x = element_text(angle = 45))

# Covariate balance check of treatment: Student status by treatment
# gg_stud_color <- dt[student_status %in% c("Yes", "No")] %>%
#   ggplot(aes(x = student_status, fill = factor_color)) + 
#     geom_bar(position = "dodge") + labs(x = "Student Status", y = "Count")
# 
# gg_stud_context <- dt[student_status %in% c("Yes", "No")] %>%
#   ggplot(aes(x = student_status, fill = factor_context)) + 
#     geom_bar(position = "dodge") + labs(x = "Student Status", y = "Count")


# Summarize plots
grid.arrange(gg_sex_color, gg_age_color, gg_edu_color, 
             gg_sex_context, gg_age_context, gg_edu_context, 
             ncol=3)
```


Each question as an aggregate yielded varying average scores. The average scores for questions 5 and 6 are very high, so there may be a potential ceiling effect. Within-subjects regression results control for differences in question number to eliminate this issue.  

```{r EDA conditions and scores, echo=FALSE, message=FALSE, warning=FALSE}

# aggregate score by question
ggplot(dt[, .(score = mean(score)), keyby = question_number], 
       aes(x = question_number, y = score, fill = question_number)) +
  geom_bar(position = "dodge", stat = "summary", fun.y = "mean") +
  labs(subtitle = "Avg Score by Question", x = "Question Number", y = "Avg Score")

```


When we split the scores between those with 'color' treatment vs. those without, we do not see a consistent uplift when applying the treatment; only questions 2 and 3 seem to show some improvement in scores. Looking at scores among different 'context' treatments, we see that scores from 'context 3' questions yielded higher than the baseline ('context 1') for question 1, 2, 3, and 5. 

```{r echo=FALSE, message=FALSE, warning=FALSE}
# # aggregate to show average scores by condition vs. boolean
dt_score_cond <- dt[, .(agg_score2 = mean(score), count = uniqueN(id)), 
                     .(question_number, factor_color, factor_context)]

# color and scores by question
gg_score_color <- dt_score_cond %>%
  ggplot(aes(x = factor_color, y = agg_score2, fill = factor_color)) + geom_boxplot() +
  facet_wrap(~ question_number, scale = "free") +
  labs(subtitle = "Color & Scores by Question", x = "Color Condition", y = "Avg Score") +
  ylim(.2, 1)

# condition and scores by question
gg_score_context <- dt_score_cond %>%
  ggplot(aes(x = factor_context, y = agg_score2, fill = factor_context)) + geom_boxplot() +
  facet_wrap(~ question_number, scale = "free") +
  labs(subtitle = "Context & Scores by Question", x = "Context Condition", y = "Avg Score") +
  ylim(.2, 1)

grid.arrange(gg_score_color, gg_score_context, ncol=2)

```


# Regression Results

```{r warning=FALSE,include=FALSE}
#question_order
assumption_df<-NA

for (participant in unique(dt$participant_id)) {
  participant_df=dt[participant_id==participant,]
  for (question in 1:6) {
  
    #see if length of dataframe filtered based on position and truth value is greater than 1
    tryCatch({score=participant_df[question_order==question,]$score #will need to update
    previous_questions=participant_df[question_order<question,]
    previous_question_color=ifelse(sum(previous_questions$color)>0,1,0)
    previous_question_cond1=ifelse(sum(previous_questions$cond1)>0,1,0)
    previous_question_cond2=ifelse(sum(previous_questions$cond2)>0,1,0)
    previous_question_cond3=ifelse(sum(previous_questions$cond3)>0,1,0)

    future_questions=participant_df[question_order>question,]
    future_question_color=ifelse(sum(future_questions$color)>0,1,0)
    future_question_cond1=ifelse(sum(future_questions$cond1)>0,1,0)
    future_question_cond2=ifelse(sum(future_questions$cond2)>0,1,0)
    future_question_cond3=ifelse(sum(future_questions$cond3)>0,1,0)
    
    participant_assumption_df=data.frame("participant_id"=c(participant),
                                         "question_order"=c(question),
                                         "score"=c(score), "previous_question_color"=c(previous_question_color),
                                         "previous_question_cond1"=c(previous_question_cond1),
                                         "previous_question_cond2"=c(previous_question_cond2),
                                         "previous_question_cond3"=c(previous_question_cond3),
                                         "future_question_color"=c(future_question_color),
                                         "future_question_cond1"=c(future_question_cond1),
                                         "future_question_cond2"=c(future_question_cond2),
                                         "future_question_cond3"=c(future_question_cond3))
    
    if (is.na(assumption_df)) {
      assumption_df=participant_assumption_df
    } else {
      assumption_df=rbind(assumption_df,participant_assumption_df,fill=TRUE)
    }},
    
    error=function(e){}
    )
  }
}

assumption_df=assumption_df[assumption_df$participant_id!=TRUE,]
```

Before using regression models to analyze the results, we review two primary assumptions of a within-subjects design, the no-anticipation and no-persistence assumptions.

## Assessment of No-Anticipation and No-Persistence Assumptions

A within-subjects design relies on the no-anticipation assumption as well as the no-persistence assumption. According to Gerber and Green (2012), "the no-anticipation assumption states that potential outcomes are unaffected by treatments that are administered in the future" and "no-persistence assumption requires that potential outcomes in one period are unaffected by treatments administered in prior periods". The treatments in this case are the two color conditions (no color, and color) as well as the question context conditions (1: no context, 2: context, and 3: context with diagram). To assess whether the no-persistence and no-anticipation assumptions associated with within-subject designs hold, the existence of future and previous conditions were regressed against question `score`. The model formula is below:

$$
\begin{aligned}
score= &future\_question\_color+previous\_question\_color+previous\_question\_context1 \\
 & +previous\_question\_context2+previous\_question\_context3+future\_question\_context1 \\
 & +future\_question\_context2+future\_question\_context3
\end{aligned}
$$


   
The summary of the model in the table below shows no previous or future treatment conditions had a significant effect on question `score`. This indicates that the no-persistence and no-anticipation assumptions of the within-subjects design hold. The model was created using `participant_id` fixed effects and clustered standard errors to control for differences amongst subjects.

```{r echo=FALSE}
assumption.mod<-lm(score~future_question_color+previous_question_color+previous_question_cond1
   +previous_question_cond2+previous_question_cond3+future_question_cond1
   +future_question_cond2+future_question_cond3+participant_id
   ,data=assumption_df)

stargazer(assumption.mod,title="Model to Assess Within-Subjects Assumptions",type="text",
          omit=c("participant_id"),
          covariate.labels = c("future.question.color","previous.question.color","previous.question.context1","previous.question.context2","previous.question.context3","future.question.context1","future.question.context2","future.question.context3","Constant"),
          se=list(Clustered_SE(model=assumption.mod,cluster=assumption_df[,"participant_id"])))
```


**Stage I: Analysing the treatment effect at the individual question level.**

In the within-subject design, since each question can be treated as a separate experiment, we analyze the treatment effect on the outcome variable (score) for each experiment. In stage I, by considering each question as a 'between-subjects' experiment we look for the average treatment effect (ATE) of color or context condition on score , which is a measure of an individuals' ability to answer analytical problems.


```{r Multiple Regression for each Question, echo=FALSE}

# for (i in 1:6) {
#   print(glue("Q{i}"))
#   mod <- dt[dt$question_number == i, lm(score ~ factor_color * factor_context)]
#   print(coeftest(mod, vcov. = vcovHC(mod) ))
# }

mod1 = dt[dt$question_number == 1, lm(score ~ factor_color * factor_context )]
mod2 = dt[dt$question_number == 2, lm(score ~ factor_color * factor_context )]
mod3 = dt[dt$question_number == 3, lm(score ~ factor_color * factor_context )]
mod4 = dt[dt$question_number == 4, lm(score ~ factor_color * factor_context )]
mod5 = dt[dt$question_number == 5, lm(score ~ factor_color * factor_context )]
mod6 = dt[dt$question_number == 6, lm(score ~ factor_color * factor_context )]


# coeftest(mod1)

stargazer(mod1, mod2, mod3, mod4, mod5, mod6,
          type = 'text',
          se = list( Robust_SE(mod1), Robust_SE(mod2), Robust_SE(mod3),
                     Robust_SE(mod4), Robust_SE(mod5), Robust_SE(mod6) ),
          style = 'qje',
          title = "OLS regression of treatment effect for each question",
          dep.var.caption  = "Participant test score", 
          dep.var.labels   = "",
          model.numbers = F,
          digits = 3,
          intercept.bottom = FALSE,
          omit = c('id', "question_number"),
          column.labels = c('question 1', 'question 2', 'question 3', 'question 4', 'question 5', 'question 6'),
          header=T,
          # report = "vcp",
          float = FALSE,
          omit.stat = c("rsq", "f","ser"),
          notes.align = "c",
          notes.label = "Stat. significance (p)",
          covariate.labels=c("Constant","color","context2","context3","color:context2","color:context3")

) 

```

- We find that the treatment of color has no significant treatment effect for any of the question
- However, the treatment effect for condition *3: Problem description with context and diagram* is statistically significant for questions 1, 3 and 4.

This is an encouraging result of the treatment effect on the outcome 'score' and we choose the explore the study results at the aggregate level by pooling the data together.


**Stage II: Pool the data and test the treatment effect by considering 3 classes of models:**

## Framework for the analysis of results

According to Gerber and Green (2012), "the allure of within-subject design is their capacity to generate precise treatment estimates with a single subject" (p 273). Within-subjects regression requires including individual subject level fixed effect (using id which is unique to each participant). In the EDA we discovered that the average scores for questions 5 and 6 are very high, so there may be a potential ceiling effects. To control for the differences in question number we decided to also incorporate question number as a fixed effect in the model. We also test for the presence of HTE by including a interaction term. We do this using the following hierarchy of linear models:

1. **Simple model**: outcome ~ Treatment conditions
2. **Fixed effects model**: Outcome ~ Treat + Fixed Effects
3. **Fully saturated model**: Outcome ~ Treat + Fixed Effects + Interaction effects


## Statistical significance and reporting standard errors

In within-subjects studies, “the key assumption is that the errors are uncorrelated across clusters while errors for individuals belonging to the same cluster may be correlated” (Cameron and Miller [2015], p. 320). Clustering is an experimental design feature because the outcome is correlated at the suject level since we observe the same individual's reponse to multiple questions, therefore we must cluster the results using subject id to ensure that the statistical significance of treatment effects are not overstated. The stargazer model results using clustered standard errors (clustered by id).


```{r Analytical Framework models, include=FALSE}

# Analytical framework build on increasing complexity of model parameters
m1 <- dt[ , lm(score ~  factor_color + factor_context)]
m2 <- dt[ , lm(score ~  factor_color + factor_context + as.factor(id) + as.factor(question_number))]
m3 <- dt[ , lm(score ~  factor_color + factor_context + as.factor(id) + as.factor(question_number)
               + factor_color * factor_context ) ]

# coeftest(m3)
```


```{r Stargazer result1, echo=FALSE}

# Model output in stargazer

# 1.Clustering SE by participant id
stargazer(m1, m2, m3, 
          type = 'text',
          se = list( Clustered_SE(m1, cluster = dt[ , id]),
                    Clustered_SE(m2, cluster = dt[ , id]),
                    Clustered_SE(m3, cluster = dt[ , id]) ),
          style = 'qje',
          title = "OLS regression results with std errors clustered by participant ID",
          dep.var.caption  = "Participant test score", 
          dep.var.labels   = "",
          model.numbers = F,
          digits = 3,
          intercept.bottom = FALSE,
          omit = c('id', "question_number"),
          column.labels = c('[SIMPLE MODEL]', '[FIXED EFFECT MODEL]', 
                              '[FULLY SATURATED MODEL]'),
          add.lines = list( c('Fixed Effects', 'NO', 'YES', 'YES' )),
          header=T,
          # report = "vcp",
          omit.stat = c("rsq", "f","ser"),
          notes.align = "c",
          notes.label = "Stat. significance (p)",
          covariate.labels=c("Constant","color","context2","context3","color:context2","color:context3")

) 

```

Typically, the motivation given for the clustering adjustments is that unobserved components in outcomes for units within clusters are correlated. However, because correlation may occur across more than one dimension ( for e.g. subject id and question number). Clustering standard errors by both dimensions of 'id' and 'question number' will produce results under a more conservative reporting standard, ensuring that the significnace of the treatment effects are not overstated. We take the view that this second perspective best fits the experimental setting our of within-subjects study where clustering adjustments are used. Below, we present a stargazer output of the same hierarchy of linear models using clustered standard errors ( clustered by both id and question number).


```{r Stargazer result2, echo=FALSE}

# 2.Clustering SE by both participant id and question number
stargazer(m1, m2, m3,
          type = 'text',
          se = list( Clustered_SE(m1, cluster = dt[ , .(id,question_number)]),
                    Clustered_SE(m2, cluster = dt[ , .(id,question_number)]),
                    Clustered_SE(m3, cluster = dt[ , .(id,question_number)]) ),
          style = 'qje',
          title = "OLS regression results with std errors clustered by both participant ID and question number",
          dep.var.caption  = "Participant test score", 
          dep.var.labels   = "",
          model.numbers = F,
          digits = 3,
          intercept.bottom = FALSE,
          omit = c('id', "question_number"),
          column.labels = c('[SIMPLE MODEL]', '[FIXED EFFECT MODEL]', 
                              '[FULLY SATURATED MODEL]'),
          add.lines = list( c('Fixed Effects', 'NO', 'YES', 'YES' )),
          header=T,
          # report = "vcp",
          omit.stat = c("rsq", "f","ser"),
          notes.align = "c",
          notes.label = "Stat. significance (p)",
          covariate.labels=c("Constant","color","context2","context3","color:context2","color:context3")

)

```



**The findings of the analysis and the interpretation of the model results:**

> 1. The 'simple model' shows that the treatment condition *3: Problem description with context and diagram* is statistically significant but the test is able to explain very little changes in outcome due to treatment  low adj. R-squared (~0%)
> 2. The more complex 'Fixed effects model' correctly captures the within-subject variation at the participant levels and produces a etter fitting model with a higher adj. R-squared (37%) and a statistically significant treatment effect for condition 3. 
> 3. The 'fully saturated model' which nests the 'fixed effects model' aims to test for additional interaction effects between the color and context treatment conditions. The model output shows that there is no heterogeneous treatment effect (HTE) between color and context and surprisingly also no statistically significant treatment effect when the interaction term is included in the model. This makes us skepticlal of the results from the simpler nested model and conclude that the potential treatments effects might not actually exist.  
> 4. We decided to re-run all 3 models with more stringent standard errors (clustered by both id and question number). To our surprise we found that none of the model results were statistically significant and the model inference is that none of the treatment effects are statistically significant.


# Estimating Statistical Power

Power analysis is an important aspect of experimental design. Statistical power or the power of a hypothesis test is the probability that the test correctly rejects the null hypothesis. It can be used as a tool to estimate the number of observations or sample size required in order to detect an effect in an experiment.

In general, within-subject design results in greater statistical power than between-subject designs as in within-subject designs, each participant receives multiple conditions or treatments. In our example, by having 2x3 or 6 types of questions or conditions, we get 1560 observations upon 260 participants. 

Though it is true that we need fewer participants in a within-subject design in order to find statistically significant effects, the power of the study depends on the average ATE as well as the standard deviation of the ATE. The smaller the ATE and the larger the standard deviations, the lower the power of the test. Our test is under-powered due to the budgetary constraints. We are uncertain that we have enough power to detect any real treatment effects in this study. I would be ideal to collect more samples or add more questions in order to get a higher power for this experiment. In the below graph, blue lines represent the number of samples needed to have 80% power for our overall study, and the red lines delineate the current level of power in our study given the number of samples per treatment condition.

```{r, include=FALSE}

# To use for power calculation
dt[, .('mu_score' = mean(score)), keyby = .(factor_context, color)]

score_sd_1 = sd(dt[factor_context==1 & color == 0, score])
score_sd_3 = sd(dt[factor_context==3 & color == 1, score])

print(score_sd_1)
print(score_sd_3)

```

```{r power function, include=FALSE,warning=FALSE,echo=FALSE}
power_test_t <- function(
  mean_control = 0.6341, 
  mean_treat = 0.6962, 
  sd_control = 0.4827, 
  sd_treat = 0.4608,
  number_per_condition = 2000, 
  power_loops = 1000, 
  ri_loops = 1000, 
  verbose = TRUE) { 

    p_values <- NA  
    ri <- NA 
    d <- data.table('id'= 1:(number_per_condition*2))
    
    #d$condition=rep(c('control', 'treatment'), each = number_per_condition)
  
    d[, condition := rep(c('control', 'treatment'), each = number_per_condition)]  
  
    for(power_loop in 1:power_loops) { 
      if(verbose == TRUE) {
        if(power_loop %% 10 == 0) {
          cat(sprintf('Loop Number: %.0f\n', power_loop))
        }
      } 
      
      p_values[power_loop] <- t.test(
        x = rnorm(number_per_condition, mean = mean_control, sd = sd_control), 
        y = rnorm(number_per_condition, mean = mean_treat, sd = sd_treat)
      )$p.value
    }
      
    return(list(
      'p_values' = p_values, 
      'power' = mean(p_values < 0.05)
      ))
}
```


```{r message=FALSE, warning=FALSE,echo=FALSE,results='hide',fig.keep='all'}

# Increasing sample size
samples_per_condition <- c(10, 20, 50, 100, 150, 200, 260, 300, 500, 750, 1000, 1500, 2000, 2500, 3000)

size_power <- NA 

for(i in 1:length(samples_per_condition)) { 
  size_power[i] <- power_test_t(
    mean_control = 0.634 , mean_treat = 0.696,
    power_loops = 1000, verbose = FALSE,
    number_per_condition = samples_per_condition[i]
    )$power
}

plot(x = samples_per_condition, y = size_power, type = 'l', 
     main = "Sample Size and Power", ylab = "Power", xlab = "Samples per Condition") +
  abline(h = 0.8, col = 'blue') +
  abline(v = 920, col = 'blue') +
  abline(h = 0.33, col = 'red') +
  abline(v = 260, col = 'red')
```



# Limitations of within-subjects Design

**A potential limitation of the within-subjects research design is the problem of “carryover effect” and “learning effect”.**

- **Carryover effect**: In this experiment where each subject is administered 6 analytical questions, with multiple conditions, the participants may fatigue as they approach the end of the study and start to lose focus. This could potentially decrease their performance on the last questions in the study as compared to the earlier questions in the study. 
- **Learning effect**: As the subject progresses through the study, the practice effect might mean that they are more confident and accomplished after the first treatment condition, and the experience of having solved some analytical problems makes them more confident and better prepared to answer later questions in the study. 

We account for “carryover effect” and “learning effect” by using counterbalance as part of the design. We implement counterbalancing where the order of thequestiona and the order of the treatments is varied. i.e. we randomized the order of the 6 questions to ensure that the order in which the questions are administered is fully randomized. At the same time we also randomize the treatment condition so each condition is different and the subject doesn’t get the opportunity to get familiar or become practiced at answering analytical questions.

# Advantages of within-subjects design

- A major advantage of within-subjects design is it eliminates all problems concerning individual differences, using a person as a control for himself reduces variance and produces the ultimate paired-subjects design as the same subject receives both control and treatment conditions. Another important advantage that the within subject design has over the between subject design is that it requires fewer participants, to have adequately powered-tests making the process much more streamlined and less resource heavy.

- For example, in this study we chose to test 6 conditions, using 6 types of questions for 260 participants. By treating each round of questions as a separate independent experiment we were able to get 1560 observations for this study, boosting the power of the results. Ease was not the only advantage, through carefully planned within subject design we were able to implement counterbalancing to lower the possibility of individual differences skewing the results.

# Generalizability concerns

One factor that can affect the usefulness of our study, regardless of the strength of the design, is its generalizability. We believe the results of our study can be generalized across a large population but it's restricted to a very specific condition of analytical questions. We would be careful about extending the finding of the study outside the scope of the conditions in the context of online learning application. For e.g., the study results would be difficult to extend to reading comprehension or other non-analytical problems since the diagram and context conditions are not easily applicable beyond the specific scope of the analytical problems which we tested.


# Conclusion

At the close of this study, it was found that there was no significant treatment effect for the color conditions. The third context condition, question with context plus diagram, appeared to be statistically significant, but true treatment effects may not actually exist when more conservative reporting standards are used for measuring treatment effects. Therefore, we fail to reject both null hypothesis:

> 1. *Color* has no effect on a subjects' ability to answer an analytical question
> 2. *Additional context* to the question has no effect on subjects' ability to answer an analytical question


We conclude that the "use of color and/or the use of questions with context cannot be proven to help students improve analytical problem solving performance". In order to have more confidence in the results of a similar study, it would be beneficial to increase the number of subjects involved in the study or the number of the questions in the study to increase the power of the tests. When adding more questions to increase the power of effects, it is necessary to further assess the possibility of no-persistence or no-anticipation violations. According to Gerber and Green (2012) "when both assumption hold, the within subjects desgin provides unbiased estimates of the ATE" (pg 275). Without a violation of the no-anticipation and no-persistence assumptions, the within-subjects design also leads to a higher power associated with treatment effects while being able to utilize the results of fewer subjects than the alternative between-subjects design.

\newpage
## References:

Barkow, J. H., Cosmides, L., & Tooby, J.  (1992). The Adapted Mind: Evolutionary Psychology and the Generation of Culture. Oxford University Press, USA.

Garin O. (2014) Ceiling Effect. In: Michalos A.C. (eds) Encyclopedia of Quality of Life and Well-Being Research. Springer, Dordrecht. https://doi.org/10.1007/978-94-007-0753-5_296

Gerber, Alan S., and Donald P. Green. *Field Experiments: Design, Analysis, and Interpretation.* Norton &amp; c., 2012.

Miller, G. A. (1956). The magical number seven, plus or minus two: some limits on our capacity for processing information. Psychological Review, 63(2), 81–97. https://doi.org/10.1037/h0043158

Wason, P. C. (1966). In B. Foss (ed.), New Horizons in Psychology. Harmondsworth: Penguin Books. pp. 135-151.



\newpage
## Appendix:

A1 through A6 in the appendix have the different questions asked in the full study with all potential conditions listed. The question conditions are listed in the following order per question:
1. No color, no context
2. Color, no context
3. No color, context
4. Color, context
5. No color, context with diagram
6. Color, context with diagram

Each question was asked with five multiple choice answers available. These options are listed after the questions with their conditions are listed.

### A1, Question 1
\begin{figure}[h!]
\includegraphics[width=5cm, height=5cm]{questions/Q1_Condition1_BW.png}
\caption{Question 1, No Color, No Context:}
\end{figure}
\begin{figure}[h!]
\includegraphics[width=5cm, height=5cm]{questions/Q1_Condition1_Color.png}
\caption{Question 1, With Color, No Context:}
\end{figure}
\begin{figure}[h!]
\includegraphics[width=5cm, height=5cm]{questions/Q1_Condition2_BW.png}
\caption{Question 1, No Color, With Context:}
\end{figure}
\begin{figure}[h!]
\includegraphics[width=5cm, height=5cm]{questions/Q1_Condition2_Color.png}
\caption{Question 1, With Color, With Context:}
\end{figure}
\begin{figure}[h!]
\includegraphics[width=5cm, height=5cm]{questions/Q1_Condition3_BW.png}
\caption{Question 1, No Color, With Context Plus Diagram:}
\end{figure}
\begin{figure}[h!]
\includegraphics[width=5cm, height=5cm]{questions/Q1_Condition3_Color.png}
\caption{Question 1, With Color, With Context Plus Diagram:}
\end{figure}

#### Answer Options:
* 1/36
* 1/24
* 1/6
* 1/9 (Correct)
* 1/18

### A2, Question 2
\begin{figure}[h!]
\includegraphics[width=5cm, height=5cm]{questions/Q2_Condition1_BW.png}
\caption{Question 2, No Color, No Context:}
\end{figure}
\begin{figure}[h!]
\includegraphics[width=5cm, height=5cm]{questions/Q2_Condition1_Color.png}
\caption{Question 2, With Color, No Context:}
\end{figure}
\begin{figure}[h!]
\includegraphics[width=5cm, height=5cm]{questions/Q2_Condition2_BW.png}
\caption{Question 2, No Color, With Context:}
\end{figure}
\begin{figure}[h!]
\includegraphics[width=5cm, height=5cm]{questions/Q2_Condition2_Color.png}
\caption{Question 2, With Color, With Context:}
\end{figure}
\begin{figure}[h!]
\includegraphics[width=5cm, height=5cm]{questions/Q2_Condition3_BW.png}
\caption{Question 2, No Color, With Context Plus Diagram:}
\end{figure}
\begin{figure}[h!]
\includegraphics[width=5cm, height=5cm]{questions/Q2_Condition3_Color.png}
\caption{Question 2, With Color, With Context Plus Diagram:}
\end{figure}

#### Answer Options:
* 1/3
* 15/32 (Correct)
* 3/16
* 5/16
* 1/2

### A3, Question 3
\begin{figure}[h!]
\includegraphics[width=5cm, height=5cm]{questions/Q3_Condition1_BW.png}
\caption{Question 3, No Color, No Context:}
\end{figure}
\begin{figure}[h!]
\includegraphics[width=5cm, height=5cm]{questions/Q3_Condition1_Color.png}
\caption{Question 3, With Color, No Context:}
\end{figure}
\begin{figure}[h!]
\includegraphics[width=5cm, height=5cm]{questions/Q3_Condition2_BW.png}
\caption{Question 3, No Color, With Context:}
\end{figure}
\begin{figure}[h!]
\includegraphics[width=5cm, height=5cm]{questions/Q3_Condition2_Color.png}
\caption{Question 3, With Color, With Context:}
\end{figure}
\begin{figure}[h!]
\includegraphics[width=5cm, height=5cm]{questions/Q3_Condition3_BW.png}
\caption{Question 3, No Color, With Context Plus Diagram:}
\end{figure}
\begin{figure}[h!]
\includegraphics[width=5cm, height=5cm]{questions/Q3_Condition3_Color.png}
\caption{Question 3, With Color, With Context Plus Diagram:}
\end{figure}

#### Answer Options:
* 1/50
* 1/20
* 1/10 (correct)
* 1/5
* 1/25

### A4, Question 4
\begin{figure}[h!]
\includegraphics[width=5cm, height=5cm]{questions/Q4_Condition1_BW.png}
\caption{Question 4, No Color, No Context:}
\end{figure}
\begin{figure}[h!]
\includegraphics[width=5cm, height=5cm]{questions/Q4_Condition1_Color.png}
\caption{Question 4, With Color, No Context:}
\end{figure}
\begin{figure}[h!]
\includegraphics[width=5cm, height=5cm]{questions/Q4_Condition2_BW.png}
\caption{Question 4, No Color, With Context:}
\end{figure}
\begin{figure}[h!]
\includegraphics[width=5cm, height=5cm]{questions/Q4_Condition2_Color.png}
\caption{Question 4, With Color, With Context:}
\end{figure}
\begin{figure}[h!]
\includegraphics[width=5cm, height=5cm]{questions/Q4_Condition3_BW.png}
\caption{Question 4, No Color, With Context Plus Diagram:}
\end{figure}
\begin{figure}[h!]
\includegraphics[width=5cm, height=5cm]{questions/Q4_Condition3_Color.png}
\caption{Question 4, With Color, With Context Plus Diagram:}
\end{figure}

#### Answer Options
* A
* B
* C
* D
* E (correct)

### A5, Question 5
\begin{figure}[h!]
\includegraphics[width=5cm, height=5cm]{questions/Q5_Condition1_BW.png}
\caption{Question 5, No Color, No Context:}
\end{figure}
\begin{figure}[h!]
\includegraphics[width=5cm, height=5cm]{questions/Q5_Condition1_Color.png}
\caption{Question 5, With Color, No Context:}
\end{figure}
\begin{figure}[h!]
\includegraphics[width=5cm, height=5cm]{questions/Q5_Condition2_BW.png}
\caption{Question 5, No Color, With Context:}
\end{figure}
\begin{figure}[h!]
\includegraphics[width=5cm, height=5cm]{questions/Q5_Condition2_Color.png}
\caption{Question 5, With Color, With Context:}
\end{figure}
\begin{figure}[h!]
\includegraphics[width=5cm, height=5cm]{questions/Q5_Condition3_BW.png}
\caption{Question 5, No Color, With Context Plus Diagram:}
\end{figure}
\begin{figure}[h!]
\includegraphics[width=5cm, height=5cm]{questions/Q5_Condition3_Color.png}
\caption{Question 5, With Color, With Context Plus Diagram:}
\end{figure}

#### Answer Options:
* 1/4
* 3/10
* 3/8 (correct)
* 2/5
* 1/2

### A6, Question 6
\begin{figure}[h!]
\includegraphics[width=5cm, height=5cm]{questions/Q6_Condition1_BW.png}
\caption{Question 6, No Color, No Context:}
\end{figure}
\begin{figure}[h!]
\includegraphics[width=5cm, height=5cm]{questions/Q6_Condition1_Color.png}
\caption{Question 6, With Color, No Context:}
\end{figure}
\begin{figure}[h!]
\includegraphics[width=5cm, height=5cm]{questions/Q6_Condition2_BW.png}
\caption{Question 6, No Color, With Context:}
\end{figure}
\begin{figure}[h!]
\includegraphics[width=5cm, height=5cm]{questions/Q6_Condition2_Color.png}
\caption{Question 6, With Color, With Context:}
\end{figure}
\begin{figure}[h!]
\includegraphics[width=5cm, height=5cm]{questions/Q6_Condition3_BW.png}
\caption{Question 6, No Color, With Context Plus Diagram:}
\end{figure}
\begin{figure}[h!]
\includegraphics[width=5cm, height=5cm]{questions/Q6_Condition3_Color.png}
\caption{Question 6, With Color, With Context Plus Diagram:}
\end{figure}

#### Answer Options:
* Karish / K
* Asha / A
* Mohtarma / M
* Swati / S
* Gauri / G


Appendices A7 through A9 show the three questions that were not included in the full study. Images are listed with the three context conditions in the following order: no context, context, and context with diagram. Answer options are available in question images.
 
### A7

A7 in the appendix lists question 1 associated with both version 1 and version 2 of the pilot study.

![Pilot Question 1, No Color, No Context:](questions\Q1_pilot_Condition1_BW.jpg)\ 
![Pilot Question 2, No Color, No Context:](questions\Q1_pilot_Condition2_BW.jpg)\ 
![Pilot Question 3, No Color, No Context:](questions\Q1_pilot_Condition3_BW.jpg)\ 

### A8
![Omitted Question 2, No Color, No Context:](questions\A8_1.jpg)\ 
![Omitted Question 2, No Color, Context:](questions\A8_2.jpg)\ 
![Omitted Question 2, No Color, Context with Diagram:](questions\A8_3.jpg)\ 

### A9
![Omitted Question 3, No Color, No Context:](questions\A9_1.jpg)\ 
![Omitted Question 3, No Color, Context:](questions\A9_2.jpg)\ 
![Omitted Question 3, No Color, Context with Diagram:](questions\A9_3.jpg)\ 

### A10
```{r,echo=FALSE}
s2.q1.mod<-df2[,lm(correct~question_number_factor==1)]
s2.q2.mod<-df2[,lm(correct~question_number_factor==2)]
s2.q3.mod<-df2[,lm(correct~question_number_factor==3)]

stargazer(s2.q1.mod,s2.q2.mod,s2.q3.mod,
          type="text", title="Question Models",
          se=list(c(Clustered_SE(model=s2.q1.mod,cluster= df2[ , ResponseId]),
                    Clustered_SE(model=s2.q2.mod,cluster= df2[ , ResponseId]),
                    Clustered_SE(model=s2.q3.mod,cluster= df2[ , ResponseId])
          )))
```

### A11

```{r,echo=FALSE}
for (cov in c('age','education')) {
  cov=glue("{cov}")
  color_mod<-lm(shared_df$color~shared_df[[cov]])#+shared_df$ResponseId)
  condition_mod<-lm(shared_df$condition~shared_df[[cov]])#+shared_df$ResponseId)
  stargazer(title=glue("{cov} models:"),color_mod,condition_mod,
            omit="ResponseId",
            type="text")
}
```
